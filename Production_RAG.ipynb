{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNk/xdTC8PtDUElC/1TCjls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhashkar1987/Production-Rag-from-Scratch/blob/main/Production_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "  print(\"[INFO] Runing in Google colab, Installing Requirement\")\n",
        "  #!pip install -U torch\n",
        "  !pip install PyMuPDF\n",
        "  !pip install tqdm\n",
        "  #!pip install sentence-transformer\n",
        "  !pip install accelerate\n",
        "  !pip install bitsandbytes\n",
        "  !pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "6OqU6rJ-77lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio transformers sentence-transformer\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -U transformers sentence-transformer"
      ],
      "metadata": {
        "id": "Jw3hhNjRIwH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download file\n",
        "import os\n",
        "import requests\n",
        "\n",
        "#Get PDF document\n",
        "pdf_path = \"human-nutrition-text.pdf\"\n",
        "\n",
        "if not os.path.exists(pdf_path):\n",
        "  print(\"File doesn't exist, Downloading...\")\n",
        "  #The url of the documnet which you are trying to download\n",
        "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
        "  # The local filename to save the downloaded file\n",
        "  filename = pdf_path\n",
        "  # Send a get requet to URL\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    #Open a file in binary write mode and save the content to it\n",
        "    with open(filename, \"wb\") as file:\n",
        "      file.write(response.content)\n",
        "    print(f\"The file has been downloaded and saved as {filename}\")\n",
        "  else:\n",
        "    print(f\"failed to download the file. Status code : {response.status_code} \")\n",
        "\n",
        "else:\n",
        "  print(f\"File {pdf_path} exists\")"
      ],
      "metadata": {
        "id": "O-OEAf0QMJQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def text_formatter(text: str) -> str:\n",
        "    \"\"\"Performs minor formatting on text\"\"\"\n",
        "    return text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
        "    \"\"\"\n",
        "    docs = fitz.open(pdf_path)\n",
        "    pages_and_texts = []\n",
        "\n",
        "    for page_number, page in tqdm(enumerate(docs), total=len(docs)):\n",
        "        text = page.get_text()\n",
        "        text = text_formatter(text)\n",
        "        pages_and_texts.append({\n",
        "            \"page_number\": page_number - 41,   # adjust offset if needed\n",
        "            \"page_char_count\": len(text),\n",
        "            \"page_word_count\": len(text.split(\" \")),\n",
        "            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "            \"page_token_count\": len(text) // 4,\n",
        "            \"text\": text\n",
        "        })\n",
        "\n",
        "    return pages_and_texts\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
        "pages_and_texts[:2]\n"
      ],
      "metadata": {
        "id": "fY2GIGtWWymG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.sample(pages_and_texts, k=3)"
      ],
      "metadata": {
        "id": "1QstHvl3foqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "VhyeAbhVf_VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "9cUJzqRlgTNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text: str, chunk_size: int = 500) -> list:\n",
        "  \"\"\"\n",
        "  Splits text into chunks of approx . `chunk size` characters.\n",
        "  \"\"\"\n",
        "  chunks = []\n",
        "  current_chunk = ''\n",
        "  words = text.split()\n",
        "\n",
        "  for word in words:\n",
        "    #check if adding the words exceeds chunk size\n",
        "    if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
        "      current_chunk += (word + ' ')\n",
        "    else:\n",
        "      # Stroe Current chunk and start new one\n",
        "      chunks.append(current_chunk.strip())\n",
        "      current_chunk = word + ' '\n",
        "\n",
        "  # Add the last chunk if not empty\n",
        "\n",
        "  if current_chunk:\n",
        "    chunks.append(current_chunk.strip())\n",
        "\n",
        "  return chunks\n",
        "\n",
        "\n",
        "def chunk_pdf_pages(pages_and_texts: list, chunk_size: int = 500) -> list[dict]:\n",
        "  \"\"\"\n",
        "  Takes PDF pages with text and splits them into chnks\n",
        "\n",
        "  Return a list of dicts with page_number, chunk_index, and chunk_text.\n",
        "  \"\"\"\n",
        "\n",
        "  all_chunks = []\n",
        "  for page in pages_and_texts:\n",
        "    page_number = page[\"page_number\"]\n",
        "    page_text = page[\"text\"]\n",
        "\n",
        "    chunks = chunk_text(page_text, chunk_size=chunk_size)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "      all_chunks.append({\n",
        "          \"page_number\": page_number,\n",
        "          \"chunk_index\": i,\n",
        "          \"chunk_char_count\": len(chunk),\n",
        "          \"chunk_word_count\": len(chunk.split()),\n",
        "          \"chunk_token_count\": len(chunk) / 4,\n",
        "          \"chunk_text\": chunk\n",
        "\n",
        "      })\n",
        "  return all_chunks\n",
        "\n",
        "#Example usage\n",
        "chunked_pages = chunk_pdf_pages(pages_and_texts, chunk_size=500)\n",
        "print(f\"Total cunks: {len(chunked_pages)}\")\n",
        "print(f\"First chunk (page {chunked_pages[0]['page_number']}): {chunked_pages[0]['chunk_text'][:200]}...\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GeWVz3sTAxZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Chuking :**\n",
        "Fixed Size Chunking\n",
        "Semantic Chunking\n",
        "Recursive Chunking\n",
        "Structural Chunking\n",
        "LLm CHunking"
      ],
      "metadata": {
        "id": "kv_39RCp4_vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random, textwrap\n",
        "\n",
        "\n",
        "#----------------------Sampling & preety Printing -----------------------------\n",
        "\n",
        "def _scattered_indices(n: int, k: int, jitter_frac = 0.08) -> list[int]:\n",
        "  \"\"\"Evenly Spaced anchors + random jitter -> indices scattered accros [0, n-1 ].\"\"\"\n",
        "  if k <= 0:\n",
        "    return []\n",
        "  if k == 1:\n",
        "    return [random.randrange(n)]\n",
        "\n",
        "  anchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
        "  out, seen = [], set()\n",
        "  radius = max(1, int(n * jitter_frac))\n",
        "  for a in anchors:\n",
        "    lo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
        "    j = random.randint(lo, hi)\n",
        "    if j not in seen:\n",
        "      out.append(j); seen.add(j)\n",
        "\n",
        "  while len(out) < k:\n",
        "    r = random.randrange(n)\n",
        "    if r not in seen:\n",
        "      out.append(r); seen.add(r)\n",
        "  return out\n",
        "\n",
        "\n",
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    header = (\n",
        "        f\" Chunk p{c['page_number']}  . idx {c['chunk_index']}   |\"\n",
        "        f\" chars {c['chunk_char_count']}  .  words {c['chunk_word_count']}  . ~ tokens {c['chunk_token_count']} \"\n",
        "    )\n",
        "\n",
        "    # Wrap body texts, avoid breaking long words awkwardly\n",
        "    wrapped_lines = textwrap.wrap(\n",
        "        c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "    )\n",
        "\n",
        "    content_width = max([0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), content_width + 2)\n",
        "\n",
        "    top    = \" \" + \"=\" * box_width + \" \"\n",
        "    hline  = \" \" + header.ljust(box_width) + \" \"\n",
        "    sep    = \" \" + \"_\" * box_width + \" \"\n",
        "    body   = \"\\n\".join(\" \" + line.ljust(box_width - 2) + \" \" for line in wrapped_lines) or \\\n",
        "             (\" \" + \"\".ljust(box_width - 2) + \" \")\n",
        "    bottom = \" \" + \"=\" * box_width + \" \"\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "\n",
        "\n",
        "def show_random_chunks(pages_and_texts: list, chunk_size: int = 500, k: int = 5, seed: int | None = 42):\n",
        "  if seed is not None:\n",
        "    random.seed(seed)\n",
        "  all_chunks = chunk_pdf_pages(pages_and_texts, chunk_size=chunk_size)\n",
        "  if not all_chunks:\n",
        "    print(\"No chunk to diplay.\")\n",
        "    return\n",
        "  idxs = _scattered_indices(len(all_chunks), k)\n",
        "  print(f\"Showing {len(idxs)} scattered random chunks out of {len(all_chunks)} total:\\n\")\n",
        "  for i, idx in enumerate(idxs, 1):\n",
        "    print(f\"#{i}\")\n",
        "    print(_draw_boxed_chunk(all_chunks[idx]))\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "#----------------Run---------------------------\n",
        "assert 'pages_and_texts' in globals(), \"Run: pages_and_texts = open_and_read_pdf(pdf_path) frist.\"\n",
        "show_random_chunks(pages_and_texts, chunk_size=500, k=5, seed=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ah4e7eeoJLfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random, textwrap\n",
        "\n",
        "\n",
        "#----------------------Sampling & preety Printing -----------------------------\n",
        "\n",
        "def _scattered_indices(n: int, k: int, jitter_frac = 0.08) -> list[int]:\n",
        "  \"\"\"Evenly Spaced anchors + random jitter -> indices scattered accros [0, n-1 ].\"\"\"\n",
        "  if k <= 0:\n",
        "    return []\n",
        "  if k == 1:\n",
        "    return [random.randrange(n)]\n",
        "\n",
        "  anchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
        "  out, seen = [], set()\n",
        "  radius = max(1, int(n * jitter_frac))\n",
        "  for a in anchors:\n",
        "    lo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
        "    j = random.randint(lo, hi)\n",
        "    if j not in seen:\n",
        "      out.append(j); seen.add(j)\n",
        "\n",
        "  while len(out) < k:\n",
        "    r = random.randrange(n)\n",
        "    if r not in seen:\n",
        "      out.append(r); seen.add(r)\n",
        "  return out\n",
        "\n",
        "\n",
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    header = (\n",
        "        f\" Chunk p{c['page_number']}  . idx {c['chunk_index']}   |\"\n",
        "        f\" chars {c['chunk_char_count']}  .  words {c['chunk_word_count']}  . ~ tokens {c['chunk_token_count']} \"\n",
        "    )\n",
        "\n",
        "    # Wrap body texts, avoid breaking long words awkwardly\n",
        "    wrapped_lines = textwrap.wrap(\n",
        "        c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "    )\n",
        "\n",
        "    content_width = max([0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), content_width)\n",
        "\n",
        "    top    = \"┌\" + \"─\" * (box_width + 2) + \"┐\"\n",
        "    hline  = \"│ \" + header.ljust(box_width) + \" │\"\n",
        "    sep    = \"├\" + \"─\" * (box_width + 2) + \"┤\"\n",
        "    body   = \"\\n\".join(\"│ \" + line.ljust(box_width) + \" │\" for line in wrapped_lines) or \\\n",
        "             (\"│ \" + \"\".ljust(box_width) + \" │\")\n",
        "    bottom = \"└\" + \"─\" * (box_width + 2) + \"┘\"\n",
        "\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "\n",
        "\n",
        "def show_random_chunks(pages_and_texts: list, chunk_size: int = 500, k: int = 5, seed: int | None = 42):\n",
        "  if seed is not None:\n",
        "    random.seed(seed)\n",
        "  all_chunks = chunk_pdf_pages(pages_and_texts, chunk_size=chunk_size)\n",
        "  if not all_chunks:\n",
        "    print(\"No chunk to diplay.\")\n",
        "    return\n",
        "  idxs = _scattered_indices(len(all_chunks), k)\n",
        "  print(f\"Showing {len(idxs)} scattered random chunks out of {len(all_chunks)} total:\\n\")\n",
        "  for i, idx in enumerate(idxs, 1):\n",
        "    print(f\"#{i}\")\n",
        "    print(_draw_boxed_chunk(all_chunks[idx]))\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "#----------------Run---------------------------\n",
        "assert 'pages_and_texts' in globals(), \"Run: pages_and_texts = open_and_read_pdf(pdf_path) frist.\"\n",
        "show_random_chunks(pages_and_texts, chunk_size=500, k=5, seed=42)"
      ],
      "metadata": {
        "id": "VH_xXptXYaDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFzCh-d3HSJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Semantic Chunking\n",
        "\n"
      ],
      "metadata": {
        "id": "8P9POUIZOiXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade \"sentence-transformers==3.0.1\" \"transformers<5, >4.1\" scikit-learn nltk"
      ],
      "metadata": {
        "id": "bptgQXSIuxoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "#Load Once Globaly\n",
        "\n",
        "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def semantic_chunk_text(text: str, similarity_threshhold: float = 0.8, max_tokens: int = 500) -> list:\n",
        "  \"\"\"Splits text into semantic chunk base on sentence similarity and max token length \"\"\"\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  if not sentences:\n",
        "    return []\n",
        "\n",
        "  embeddings = semantic_model.encode(sentences)\n",
        "\n",
        "  chunks = []\n",
        "  current_chunk = [sentences[0]]\n",
        "  current_embedding = embeddings[0]\n",
        "\n",
        "  for i in range(1, len(sentences)):\n",
        "    sim = cosine_similarity([current_embedding], [embeddings[i]])[0][0]\n",
        "    chunk_token_count = len(\" \".join(current_chunk)) // 4\n",
        "\n",
        "    if sim >= similarity_threshhold and chunk_token_count < max_tokens:\n",
        "      current_chunk.append(sentences[i])\n",
        "      current_embedding = np.mean([current_embedding, embeddings[i]], axis=0)\n",
        "    else:\n",
        "      chunks.append(\" \".join(current_chunk))\n",
        "      current_chunk = [sentences[i]]\n",
        "      current_embedding = embeddings[i]\n",
        "\n",
        "  if current_chunk:\n",
        "   chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "  return chunks\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "def semantic_chunk_pdf_pages(pages_and_texts: list, similarity_threshhold: float = 0.8, max_tokens: int = 500) -> list[dict]:\n",
        "    \"\"\"Takes PDF pages with text and splits them into semantic chunks .\n",
        "    Returns a list of dicsts with page_number, chunk_index and chunk_text \"\"\"\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    for page in tqdm(pages_and_texts, desc=\"Sematic chunking page\"):\n",
        "      page_number = page[\"page_number\"]\n",
        "      page_text = page[\"text\"]\n",
        "\n",
        "      chunks = semantic_chunk_text(page_text,\n",
        "                                   similarity_threshhold=similarity_threshhold,\n",
        "                                   max_tokens=max_tokens)\n",
        "      for i, chunk in enumerate(chunks):\n",
        "        all_chunks.append({\n",
        "           \"page_number\": page_number,\n",
        "           \"chunk_index\": i,\n",
        "           \"chunk_char_count\": len(chunk),\n",
        "           \"chunk_word_count\": len(chunk.split()),\n",
        "           \"chunk_token_count\": len(chunk) / 4,\n",
        "           \"chunk_text\": chunk\n",
        "        })\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f96Hji-Lv59n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "semantic_chunked_pages = semantic_chunk_pdf_pages(pages_and_texts, similarity_threshhold=0.75, max_tokens=500)\n",
        "\n",
        "print(f\"Total semantic chunks : {len(semantic_chunked_pages)}\")\n",
        "print(f\"First semantic chunk : (page {semantic_chunked_pages[0]['page_number']}):\")\n",
        "print(semantic_chunked_pages[0]['chunk_text'][:200] + \"......\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dRR6QI_qBxxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random, textwrap\n",
        "\n",
        "def _scattered_indices(n: int, k: int, jitter_frac = 0.08) -> list[int]:\n",
        "  \"\"\"Evenly Spaced anchors + random jitter -> indices scattered accros [0, n-1 ].\"\"\"\n",
        "  if k <= 0:\n",
        "    return []\n",
        "  if k == 1:\n",
        "    return [random.randrange(n)]\n",
        "\n",
        "  anchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
        "  out, seen = [], set()\n",
        "  radius = max(1, int(n * jitter_frac))\n",
        "  for a in anchors:\n",
        "    lo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
        "    j = random.randint(lo, hi)\n",
        "    if j not in seen:\n",
        "      out.append(j); seen.add(j)\n",
        "\n",
        "  while len(out) < k:\n",
        "    r = random.randrange(n)\n",
        "    if r not in seen:\n",
        "      out.append(r); seen.add(r)\n",
        "  return out\n",
        "\n",
        "\n",
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    header = (\n",
        "        f\" Chunk p{c['page_number']}  . idx {c['chunk_index']}   |\"\n",
        "        f\" chars {c['chunk_char_count']}  .  words {c['chunk_word_count']}  . ~ tokens {c['chunk_token_count']} \"\n",
        "    )\n",
        "\n",
        "    # Wrap body texts, avoid breaking long words awkwardly\n",
        "    wrapped_lines = textwrap.wrap(\n",
        "        c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "    )\n",
        "\n",
        "    content_width = max([0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), content_width)\n",
        "\n",
        "    top    = \"┌\" + \"─\" * (box_width + 2) + \"┐\"\n",
        "    hline  = \"│ \" + header.ljust(box_width) + \" │\"\n",
        "    sep    = \"├\" + \"─\" * (box_width + 2) + \"┤\"\n",
        "    body   = \"\\n\".join(\"│ \" + line.ljust(box_width) + \" │\" for line in wrapped_lines) or \\\n",
        "             (\"│ \" + \"\".ljust(box_width) + \" │\")\n",
        "    bottom = \"└\" + \"─\" * (box_width + 2) + \"┘\"\n",
        "\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "\n",
        "\n",
        "def show_random_semantics_chunks(pages_and_texts: list, chunk_size: int = 500, k: int = 5, seed: int | None = 42):\n",
        "  if seed is not None:\n",
        "    random.seed(seed)\n",
        "  n = len(semantic_chunked_pages)\n",
        "  if n == 0:\n",
        "    print(\"No Semantic chunk to display .\");\n",
        "    return\n",
        "  idxs = _scattered_indices(n, k)\n",
        "  print(f\"Showing {len(idxs)} scattered Semantic chunk out of {n} total:\\n\")\n",
        "  for i, idx in enumerate(idxs, 1):\n",
        "    print(f\"#{i}\")\n",
        "    print(_draw_boxed_chunk(semantic_chunked_pages[idx]))\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "#----------------Run---------------------------\n",
        "assert 'semantic_chunked_pages' in globals() and len(semantic_chunked_pages) > 0, \\\n",
        "\"Run your semantic chunkeing code frist to define `semantic_chunked_pages` . \"\n",
        "show_random_semantics_chunks(semantic_chunked_pages, k=5, seed=42)"
      ],
      "metadata": {
        "id": "QMrKuIlnHUle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking Strategy 3: Recursive Chunking (Structure-Aware for RAG)**\n",
        "\n",
        "**What Is Recursive Chunking?**\n",
        "Recursive Chunking is a structure-preserving strategy designed to break down long documents into semantically meaningful chunks for RAG pipelines. Unlike naive splitting methods that ignore context, this approach respects the natural hierarchy of the text—starting from sections, then paragraphs, and finally sentences—ensuring that each chunk retains coherence and relevance for retrieval.\n",
        "\n",
        "**⚙️ How It Works (Step-by-Step)**\n",
        "**Initial Size Check** If the input block is already smaller than the max_chunk_size (e.g., in tokens or characters), it's retained as-is—no further splitting needed.\n",
        "\n",
        "**Split by Section Boundaries (\\n\\n)** If the block is too large, the algorithm first attempts to split it using double newlines, which typically indicate section breaks or topic shifts.\n",
        "\n",
        "**Fallback to Paragraphs (\\n)** If sections are still too long, it recursively splits further using single newlines to isolate paragraphs.\n",
        "\n",
        "**Final Fallback to Sentences** When paragraph-level chunks exceed the size limit, the algorithm falls back to sentence-level splitting using NLP tools like nltk.sent_tokenize.\n",
        "\n",
        "**Why It’s Ideal for RAG**\n",
        "Preserves Semantic Flow: Chunks align with the document’s logical structure, improving retrieval relevance.\n",
        "\n",
        "**Minimizes Fragmentation:** Avoids breaking sentences or concepts mid-way, which can degrade embedding quality.\n",
        "\n",
        "**Flexible Granularity**: Adapts chunk size dynamically based on content structure, not just token count.\n",
        "\n",
        "Step 1 : Split Hole text\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "0uDSkHU3O8wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from tqdm.auto import tqdm\n",
        "nltk.download('punkt')\n",
        "\n",
        "def recursive_chunk_text(text:str, max_chunk_size: int = 1000, min_chunk_size: int = 100) -> list:\n",
        "  \"\"\"Recursively splits a block of text into chunks that fit within size constraints\n",
        "  Tries spliting by section, then newlines, then sentences\n",
        "  \"\"\"\n",
        "\n",
        "  def split_chunk(chunk: str) -> list:\n",
        "    #Base Case\n",
        "    if len(chunk) <= max_chunk_size:\n",
        "      return [chunk]\n",
        "\n",
        "\n",
        "    #Try Spliting by double newlines\n",
        "    sections = chunk.split(\"\\n\\n\")\n",
        "    if len(sections) > 1:\n",
        "      result = []\n",
        "      for section in sections:\n",
        "        if sections.strip():\n",
        "          result.extend(split_chunk(section.strip()))\n",
        "      return result\n",
        "\n",
        "      # Try Spliting by single newline\n",
        "\n",
        "    sections = chunk.split(\"\\n\")\n",
        "    if len(sections) > 1:\n",
        "      result = []\n",
        "      for section in sections:\n",
        "        if sections.strip():\n",
        "          result.extend(split_chunk(section.strip()))\n",
        "      return result\n",
        "\n",
        "  sentences = nltk.sent_tokenize(chunk)\n",
        "  chunks, current_chunk, current_size = [], [], 0\n",
        "\n",
        "  for sentence in sentences:\n",
        "    if current_size + len(sentence) > max_chunk_size:\n",
        "      if current_chunk:\n",
        "        chunk.append(\" \".join(current_chunk))\n",
        "      current_chunk = [sentence]\n",
        "      current_size = len(sentence)\n",
        "    else:\n",
        "        current_chunk = [sentence]\n",
        "        current_size = len(sentence)\n",
        "  if current_chunk:\n",
        "    chunk.append(\" \".join(current_chunk))\n",
        "\n",
        "        return chunk\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W12rmIyJRxaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from tqdm.auto import tqdm\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def recursive_chunk_text(text: str, max_chunk_size: int = 1000, min_chunk_size: int = 100) -> list:\n",
        "    \"\"\"Recursively splits a block of text into chunks that fit within size constraints.\n",
        "    Tries splitting by section, then newlines, then sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    def split_chunk(chunk: str) -> list:\n",
        "        # Base case\n",
        "        if len(chunk) <= max_chunk_size:\n",
        "            return [chunk]\n",
        "\n",
        "        # Try splitting by double newlines\n",
        "        sections = chunk.split(\"\\n\\n\")\n",
        "        if len(sections) > 1:\n",
        "            result = []\n",
        "            for section in sections:\n",
        "                if section.strip():\n",
        "                    result.extend(split_chunk(section.strip()))\n",
        "            return result\n",
        "\n",
        "        # Try splitting by single newline\n",
        "        sections = chunk.split(\"\\n\")\n",
        "        if len(sections) > 1:\n",
        "            result = []\n",
        "            for section in sections:\n",
        "                if section.strip():\n",
        "                    result.extend(split_chunk(section.strip()))\n",
        "            return result\n",
        "\n",
        "        # Fallback to sentence-level splitting\n",
        "        sentences = nltk.sent_tokenize(chunk)\n",
        "        chunks, current_chunk, current_size = [], [], 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_len = len(sentence)\n",
        "            if current_size + sentence_len > max_chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_size = sentence_len\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_size += sentence_len\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    return split_chunk(text)\n"
      ],
      "metadata": {
        "id": "pxMXY1Iuws8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}